{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор детальной информации о предложениях по продаже квартир\n",
    "\n",
    "Этот ноутбук содержит код сбора детальной информации о предложениях по продаже квартир в новостройках Санкт-Петербурга.\n",
    "\n",
    "Для сбора информации используются наборы ссылок на страницы с предложениями от агентов и предложениями от застройщика, которые были подготовлены в ноутбуке `links_generator`.\n",
    "\n",
    "### Какая информация о предложениях собирается\n",
    "* Заголовок предложения\n",
    "* Количество комнат в квартире\n",
    "* Общая площадь квартиры\n",
    "* Этаж, на котором находится квартира\n",
    "* Общее количество этажей в здании\n",
    "* Ссылка на предложение на сайте Циан\n",
    "* Общая стоимость квартиры\n",
    "* Стоимость за метр квадратный\n",
    "* Название жк, в котором расположена квартира\n",
    "* Адрес жк\n",
    "* Дата завершения строительства ЖК\n",
    "* Ближайшая станция метро\n",
    "* Время в пути до ближайшей станции метро\n",
    "* Находится ли метро в пешей доступности (0 если время в предыдущем пункте указано на транспорте и 1, если пешком)\n",
    "\n",
    "### Важная информация, которую стоит узнать перед запуском\n",
    "\n",
    "Так как на сайте Циан установлена защита от роботов, у вас не получится собрать всю информацию за один раз. Обычно за раз с использованием этого кода удается загрузить информацию с 20-250 страниц. После этого возникает капча, обход которой у нас не предусмотрен, поэтому при ее возникновении рекомендуется подождать 20-60 минут и продолжить сбор со страницы, на которой он прервался на прошлой итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import socket\n",
    "import socks\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy setup\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socks.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции, которые используются для получения информации о предложениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for calculating total number of pages portions for given portion size and pages links pool\n",
    "# Used in offers_info_grabber function\n",
    "def get_portions_num(portion_size, pages_links_pool):\n",
    "    return math.ceil(len(pages_links_pool) / portion_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function, that extract all neccessary information about flat from the flat offer's html page (offers_part) \n",
    "# and write it to the given file (flats_info)\n",
    "def get_flat_offers_page_info(offers_part, flats_info):\n",
    "    flats_top_titles = offers_part.findAll('div', attrs = {'data-name':'TopTitle'})\n",
    "    to_get_links_top = offers_part.findAll('div', attrs = {'data-name':'TopOfferCard'})\n",
    "    to_get_links = offers_part.findAll('div', attrs = {'data-name':'OfferCard'})\n",
    "    flats_titles = offers_part.findAll('div', attrs = {'data-name':'Title'})\n",
    "    flats_deadlines = offers_part.findAll('div', attrs = {'data-name':'Deadline'})\n",
    "    complex_names = offers_part.findAll('a', attrs = {'class':'c6e8ba5398--building-link--1dQyE'})\n",
    "    subway_info = offers_part.findAll('div', attrs = {'data-name':'Underground'})\n",
    "    address_info = offers_part.findAll('div', attrs = {'data-name':'AddressItem'})\n",
    "    top_price_info = offers_part.findAll('div', attrs = {'data-name':'TopPrice'})\n",
    "    price_info = offers_part.findAll('div', attrs = {'data-name':'Price'})\n",
    "    \n",
    "    all_flats_titles_arr = []\n",
    "    for i in range(len(flats_top_titles)):\n",
    "        all_flats_titles_arr.append(flats_top_titles[i])\n",
    "    for i in range(len(flats_titles)):\n",
    "        all_flats_titles_arr.append(flats_titles[i])\n",
    "        \n",
    "    offers_num = len(all_flats_titles_arr)\n",
    "\n",
    "    flat_titles_strs = []\n",
    "    for i in range(len(all_flats_titles_arr)):\n",
    "        title_str = re.findall(r\">(.*)<\", str(all_flats_titles_arr[i]))[0]\n",
    "        flat_titles_strs.append(title_str)\n",
    "\n",
    "    address_strs = []\n",
    "    for i in range(len(address_info)):\n",
    "        address_str = address_info[i].find('span', attrs = {'itemprop':'name'}).attrs['content']\n",
    "        address_strs.append(address_str)\n",
    "        \n",
    "    # Check the length\n",
    "    if len(address_strs) < offers_num:\n",
    "        for k in range(len(address_strs), offers_num):\n",
    "            address_strs.append('')\n",
    "\n",
    "    total_prices_strs = []\n",
    "    prices_for_meter_strs = []\n",
    "    for i in range(len(top_price_info)):\n",
    "        total_price = top_price_info[i].find('div', attrs = {'class':'c6e8ba5398--header--1dF9r'})\n",
    "        total_price_str = re.findall(r\">(.*)<\", str(total_price))[0][:-2].replace(' ', '')\n",
    "        total_prices_strs.append(total_price_str)\n",
    "\n",
    "        price_for_meter = top_price_info[i].find('div', attrs = {'class':'c6e8ba5398--term--3kvtJ'})\n",
    "        price_for_meter_str = re.findall(r\">(.*)<\", str(price_for_meter))[0][:-5].replace(' ', '')\n",
    "        prices_for_meter_strs.append(price_for_meter_str)\n",
    "\n",
    "    for i in range(len(price_info)):\n",
    "        total_price = price_info[i].find('div', attrs = {'class':'c6e8ba5398--header--1df-X'})\n",
    "        total_price_str = re.findall(r\">(.*)<\", str(total_price))[0][:-2].replace(' ', '')\n",
    "        total_prices_strs.append(total_price_str)\n",
    "\n",
    "        price_for_meter = price_info[i].find('div', attrs = {'class':'c6e8ba5398--term--3kvtJ'})\n",
    "        price_for_meter_str = re.findall(r\">(.*)<\", str(price_for_meter))[0][:-5].replace(' ', '')\n",
    "        prices_for_meter_strs.append(price_for_meter_str)\n",
    "        \n",
    "    # Check the length of total_prices_strs\n",
    "    if len(total_prices_strs) < offers_num:\n",
    "        for k in range(len(total_prices_strs), offers_num):\n",
    "            total_prices_strs.append('')\n",
    "    # Check the length of prices_for_meter_strs\n",
    "    if len(prices_for_meter_strs) < offers_num:\n",
    "        for k in range(len(prices_for_meter_strs), offers_num):\n",
    "            prices_for_meter_strs.append('')\n",
    "\n",
    "    all_flats_links = []\n",
    "    for i in range(len(to_get_links_top)):\n",
    "        all_flats_links.append(to_get_links_top[i].find('a', attrs = {'class':'c6e8ba5398--header--1fV2A'}).attrs['href'])\n",
    "    for i in range(len(to_get_links)):   \n",
    "        all_flats_links.append(to_get_links[i].find('a').attrs['href'])\n",
    "        \n",
    "    # Check the length of all_flats_links\n",
    "    if len(all_flats_links) < offers_num:\n",
    "        for k in range(len(all_flats_links), offers_num):\n",
    "            all_flats_links.append('')\n",
    "\n",
    "    complex_deadlines_strs = []\n",
    "    for i in range(len(flats_deadlines)):\n",
    "        deadline_str = re.findall(r\">(.*)<\", str(flats_deadlines[i]))[0]\n",
    "        complex_deadlines_strs.append(deadline_str)\n",
    "        \n",
    "    # Check the length of complex_deadlines_strs\n",
    "    if len(complex_deadlines_strs) < offers_num:\n",
    "        for k in range(len(complex_deadlines_strs), offers_num):\n",
    "            complex_deadlines_strs.append('')\n",
    "\n",
    "    complex_names_strs = []\n",
    "    for i in range(len(complex_names)):\n",
    "        name_str = re.findall(r\">(.*)</a>\", str(complex_names[i]))[0]\n",
    "        complex_names_strs.append(name_str)\n",
    "        \n",
    "    # Check the length of complex_names_strs\n",
    "    if len(complex_names_strs) < offers_num:\n",
    "        for k in range(len(complex_names_strs), offers_num):\n",
    "            complex_names_strs.append('')\n",
    "\n",
    "    subway_names_strs = []\n",
    "    subway_time_strs = []\n",
    "    is_walking_time_arr = [0] * offers_num\n",
    "    for i in range(len(subway_info)):\n",
    "        subway_name = subway_info[i].find('div', attrs = {'class':'c6e8ba5398--underground-name--1efZ3'})\n",
    "        subway_name_str = re.findall(r\">(.*)<\", str(subway_name))[0]\n",
    "        subway_names_strs.append(subway_name_str)\n",
    "\n",
    "        subway_time = subway_info[i].find('div', attrs = {'class':'c6e8ba5398--remoteness--3bptF'})\n",
    "        subway_time_arr = re.findall(r\">(.*)</div>\", str(subway_time))[0].split('<!-- -->')\n",
    "        subway_time_str = subway_time_arr[0] + ' ' + subway_time_arr[2]\n",
    "        if subway_time_arr[4] == 'пешком':\n",
    "            is_walking_time_arr[i] = 1\n",
    "        subway_time_strs.append(subway_time_str)\n",
    "        \n",
    "    # Check the length of subway_names_strs\n",
    "    if len(subway_names_strs) < offers_num:\n",
    "        for k in range(len(subway_names_strs), offers_num):\n",
    "            subway_names_strs.append('')\n",
    "    # Check the length of subway_time_strs\n",
    "    if len(subway_time_strs) < offers_num:\n",
    "        for k in range(len(subway_time_strs), offers_num):\n",
    "            subway_time_strs.append('')\n",
    "        \n",
    "    room_numbers_strs = []\n",
    "    total_square_strs = []\n",
    "    floor_strs = []\n",
    "    total_floor_numbers_strs = []\n",
    "    for i in range(len(flat_titles_strs)):\n",
    "        flat_arr = flat_titles_strs[i].split(', ')\n",
    "        room_numbers_strs.append(flat_arr[0])\n",
    "        total_square_strs.append(flat_arr[1][:-3])\n",
    "        floor_info = flat_arr[2][:-5].split('/')\n",
    "        if len(floor_info) > 1:\n",
    "            floor_strs.append(floor_info[0])\n",
    "            total_floor_numbers_strs.append(floor_info[1])\n",
    "        else:\n",
    "            floor_strs.append(floor_info[0])\n",
    "            total_floor_numbers_strs.append(\"\")\n",
    "        \n",
    "    # Запись в файл\n",
    "    for i in range(len(flat_titles_strs)):\n",
    "        flats_info.write(flat_titles_strs[i] + ';')\n",
    "        flats_info.write(room_numbers_strs[i] + ';')\n",
    "        flats_info.write(total_square_strs[i] + ';')\n",
    "        flats_info.write(floor_strs[i] + ';')\n",
    "        flats_info.write(total_floor_numbers_strs[i] + ';')\n",
    "        flats_info.write(all_flats_links[i] + ';')\n",
    "        flats_info.write(total_prices_strs[i] + ';')\n",
    "        flats_info.write(prices_for_meter_strs[i] + ';')\n",
    "        flats_info.write(complex_names_strs[i] + ';')\n",
    "        flats_info.write(address_strs[i] + ';')\n",
    "        flats_info.write(complex_deadlines_strs[i] + ';')\n",
    "        flats_info.write(subway_names_strs[i] + ';')\n",
    "        flats_info.write(subway_time_strs[i] + ';')\n",
    "        flats_info.write(str(is_walking_time_arr[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that implements the scraping logic. Have 3 attributes:\n",
    "# - pages_links_pool - array with offers pages links\n",
    "# - portion_len - number of pages, after extracting information from which will be the long delay (40 recommended)\n",
    "# - page_id_to_start - number of page, from which we want to start scrapping. \n",
    "#   At first time it is 0 and on the next runs it is the page on which the grabber stoped last time.\n",
    "\n",
    "def offers_info_grabber(pages_links_pool, portion_len, page_id_to_start, file_to_write_result):\n",
    "    total_pages_num = len(pages_links_pool)\n",
    "    portions_num = get_portions_num(portion_len, pages_links_pool)\n",
    "    portion_num_to_start = math.floor(page_id_to_start / portion_len)\n",
    "    \n",
    "    # Start values\n",
    "    start_id = 0\n",
    "    end_id = portion_len\n",
    "    \n",
    "    # Iterating by the portions of the given size\n",
    "    for portion_id in range(portion_num_to_start, portions_num):   \n",
    "        if portion_id == portion_num_to_start:\n",
    "            start_id = page_id_to_start\n",
    "            end_id = portion_len * (portion_id + 1)\n",
    "        if portion_id == portions_num:\n",
    "            end_id = len(pages_links_pool) - 1\n",
    "        \n",
    "        # Iterating by pages in the portion\n",
    "        for i in range(start_id, end_id):\n",
    "            # Get offers page link from the pool\n",
    "            offers_page_link = pages_links_pool[i]\n",
    "            \n",
    "            # Get page content in lxml format\n",
    "            search_page = requests.get(offers_page_link.format(i))\n",
    "            search_page = search_page.text\n",
    "            search_page = BeautifulSoup(search_page, 'lxml')\n",
    "\n",
    "            # Get table with offers on the given page\n",
    "            offers_part = search_page.html.body.findAll('div', attrs = {'data-name':'Offers'})\n",
    "\n",
    "            # If we can't fing table with offers, it means that captcha appears. \n",
    "            # We write number of first page, information from which can't be read, and return\n",
    "            if(len(offers_part) == 0):\n",
    "                print('Captcha appears! Cannot find info about offers on page', i,\n",
    "                      'Start scraping process again from this page in 20-60 minutes')\n",
    "                return\n",
    "\n",
    "            # If it is no captcha, we start extracting information from the page and write it to the given file\n",
    "            offers_part = offers_part[0]\n",
    "            get_flat_offers_page_info(offers_part, file_to_write_result)\n",
    "\n",
    "            # After extracting information from each 50-th page, we'll get this message\n",
    "            if (i % 50) == 0:\n",
    "                print('Offers information from page', i, 'just loaded')\n",
    "\n",
    "        # After we finish extracting information from all pages in one portion, we change the start and end ids\n",
    "        start_id = end_id\n",
    "        end_id = start_id + portion_len\n",
    "        \n",
    "        if end_id > total_pages_num:\n",
    "            end_id = total_pages_num\n",
    "            \n",
    "        # Delay after each portion (in seconds)\n",
    "        time.sleep(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка информации о страницах с предложениями\n",
    "\n",
    "Данные о страницах с предложениями были подготовлены в ноутбуке `links_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=96&offer_type=flat&p=1',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=1',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=2',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=3',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=4',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=5',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=6',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=44193&offer_type=flat&p=7',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=25135&offer_type=flat&p=1',\n",
       " 'https://spb.cian.ru/cat.php?deal_type=sale&engine_version=2&from_developer=1&newobject=25135&offer_type=flat&p=2']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flats_info = open('dev_links.txt', 'r', encoding='utf-8')\n",
    "agents_links_pool = flats_info.readlines()\n",
    "flats_info.close()\n",
    "\n",
    "agents_links_pool = [re.findall(r\"(.*)\\n\", x)[0] for x in agents_links_pool]\n",
    "agents_links_pool[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск сбора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captcha appears! Cannot find info about offers on page 1100 Start scraping process again from this page in 20-60 minutes\n"
     ]
    }
   ],
   "source": [
    "flats_info = open('flat_offers_from_devs_info.txt', 'w', encoding='utf-8')\n",
    "# Запускаем функцию с параметрами: массив ссылок и размер порции\n",
    "offers_info_grabber(agents_links_pool, 100, 1100, flats_info)\n",
    "flats_info.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
